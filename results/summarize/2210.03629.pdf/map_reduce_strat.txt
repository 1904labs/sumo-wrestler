 This paper presents ReAct, a novel prompting method that combines reasoning and acting with language models to solve diverse language reasoning and decision making tasks. Experiments across four diverse benchmarks show that ReAct outperforms prior approaches that perform either reasoning or action generation in isolation. ReAct is evaluated on two knowledge-intensive reasoning tasks, HotpotQA and FEVER, and two interactive decision-making tasks, ALFWorld and WebShop. Results show that ReAct outperforms other baselines, such as Standard prompting, Chain-of-thought prompting, and Acting-only prompting. Additionally, the paper provides examples of success and failure modes analysis for the ReAct and CoT systems.
 